# Prompt annotations for transparency page
# Each prompt has human-readable explanations of key sections

# =============================================================================
# JUDGE PROMPTS - Quality validation
# =============================================================================

score_judge:
  name: "Score Rationale Judge"
  category: "quality_validation"
  description: "Verifies that narrative rationale logically supports the assigned scores. Catches mismatches between glowing language and low scores."
  status: "active"
  source_file: "src/judges/prompts/score_judge.txt"
  annotations:
    - section: "Score Interpretation"
      lines: "26-31"
      explanation: "Maps numeric AMAL scores (0-100) to qualitative tiers. A score of 90+ means 'exceptional' while 30-49 means 'below average'. This ensures narratives match score severity."
    - section: "Dimension Scoring"
      lines: "33-35"
      explanation: "Each dimension (Trust, Evidence, Effectiveness, Fit) is scored 0-25 points. The prompt explicitly notes that 25/25 IS a perfect score for that dimension - this prevents false positives when rationales correctly describe max scores."
    - section: "Severity Guidelines"
      lines: "59-61"
      explanation: "Issues are categorized as error (contradiction), warning (incomplete), or info (minor mismatch). Only errors block publication."

citation_judge:
  name: "Citation Verification Judge"
  category: "quality_validation"
  description: "Verifies that citations actually support their claims. Checks URL accessibility and content accuracy."
  status: "active"
  source_file: "src/judges/prompts/citation_judge.txt"
  annotations:
    - section: "Verification Criteria"
      lines: "20-23"
      explanation: "Three checks per citation: (1) Does the URL exist? (2) Does the content support the claim? (3) Are quoted facts accurate? All three must pass."
    - section: "Severity Guidelines"
      lines: "47-49"
      explanation: "Errors block publication (content contradicts claim). Warnings are tolerated (URL down but claim plausible). Info is logged only (formatting issues)."

factual_judge:
  name: "Factual Accuracy Judge"
  category: "quality_validation"
  description: "Cross-references facts in narratives against source data. Catches hallucinated numbers, dates, and claims."
  status: "active"
  source_file: "src/judges/prompts/factual_judge.txt"
  annotations:
    - section: "Verification Process"
      lines: "1-15"
      explanation: "Compares narrative claims against the actual source data that was provided to the LLM. Any claim not traceable to source data is flagged."
    - section: "Critical Fields"
      lines: "20-30"
      explanation: "Financial figures, dates, and statistics are highest priority. These are most likely to be hallucinated and most damaging if wrong."

zakat_judge:
  name: "Zakat Classification Judge"
  category: "quality_validation"
  description: "Validates zakat eligibility claims against source evidence. Prevents false zakat-eligible classifications."
  status: "active"
  source_file: "src/judges/prompts/zakat_judge.txt"
  annotations:
    - section: "Eligibility Criteria"
      lines: "1-20"
      explanation: "A charity is only zakat-eligible if they EXPLICITLY claim it on their website. We don't infer from programs or populations served."
    - section: "Asnaf Categories"
      lines: "25-40"
      explanation: "The 8 Quranic recipient categories. The judge verifies that mentioned asnaf are actually claimed by the charity, not assumed."

# =============================================================================
# PAGE EXTRACTION PROMPTS - Data collection
# =============================================================================

homepage_prompt:
  name: "Homepage Extraction"
  category: "data_extraction"
  description: "Extracts mission, target populations, and founding year from charity homepages."
  status: "active"
  source_file: "config/page_prompts.yaml"
  annotations:
    - section: "Critical Extraction Rules"
      lines: "8-15"
      explanation: "ONLY extracts explicitly stated information. Never infers or guesses. This prevents hallucination at the source data level."
    - section: "External Evaluations"
      lines: "12-13"
      explanation: "Only includes GiveWell/Charity Navigator/BBB if EXPLICITLY mentioned on the page. LLMs often hallucinate ratings."

about_prompt:
  name: "About Page Extraction"
  category: "data_extraction"
  description: "Extracts detailed mission, vision, values, and organizational information."
  status: "active"
  source_file: "config/page_prompts.yaml"
  annotations:
    - section: "Structured Fields"
      lines: "42-46"
      explanation: "Extracts into validated schema fields: mission (required), vision, values, target populations, geographic coverage."

programs_prompt:
  name: "Programs Page Extraction"
  category: "data_extraction"
  description: "Extracts program descriptions and service areas from charity program pages."
  status: "active"
  source_file: "config/page_prompts.yaml"
  annotations:
    - section: "Program Descriptions"
      lines: "53-54"
      explanation: "Extracts 1-20 program descriptions, each 20-500 characters. Programs are the core of what a charity does."

impact_prompt:
  name: "Impact Page Extraction"
  category: "data_extraction"
  description: "Extracts impact metrics, beneficiary counts, and outcome statistics."
  status: "active"
  source_file: "config/page_prompts.yaml"
  annotations:
    - section: "Impact Metrics"
      lines: "72-75"
      explanation: "Only includes numbers EXPLICITLY stated on the page. Never calculates or estimates. This is a hallucination-prone field."

donate_prompt:
  name: "Donation Page Extraction"
  category: "data_extraction"
  description: "Extracts tax-deductibility status and payment methods from donation pages."
  status: "active"
  source_file: "config/page_prompts.yaml"
  annotations:
    - section: "Payment Methods"
      lines: "90-91"
      explanation: "Structured list of accepted methods: credit card, PayPal, crypto, DAF, etc. Helps donors know how they can give."

contact_prompt:
  name: "Contact/Leadership Extraction"
  category: "data_extraction"
  description: "Extracts leadership team members, titles, and organizational structure."
  status: "active"
  source_file: "config/page_prompts.yaml"
  annotations:
    - section: "Leadership Array"
      lines: "99-104"
      explanation: "Structured extraction of name, title, and role for up to 50 team members. Used for governance assessment."

zakat_prompt:
  name: "Zakat Page Extraction"
  category: "data_extraction"
  description: "Extracts zakat eligibility claims, asnaf categories served, and scholarly endorsements."
  status: "active"
  source_file: "config/page_prompts.yaml"
  annotations:
    - section: "Eight Asnaf Categories"
      lines: "119-128"
      explanation: "Lists the 8 Quranic zakat recipient categories. Only includes categories the charity EXPLICITLY claims to serve."
    - section: "Scholarly Endorsements"
      lines: "132"
      explanation: "Only includes scholars/institutions EXPLICITLY named as endorsing zakat eligibility. LLMs frequently hallucinate endorsements."
    - section: "Inference Prevention"
      lines: "116-117"
      explanation: "Explicitly states: do NOT assume a charity accepts zakat just because they serve poor populations. Must use word 'zakat' explicitly."

# =============================================================================
# NARRATIVE GENERATION PROMPTS
# =============================================================================

baseline_narrative:
  name: "Baseline Narrative Generator"
  category: "narrative_generation"
  description: "Generates baseline evaluation narratives with Wikipedia-style citations. Core narrative seen by all users."
  status: "active"
  source_file: "baseline.py (inline, lines 324-412)"
  annotations:
    - section: "Mandatory Values"
      lines: "339-348"
      explanation: "Lists exact financial values that MUST be used verbatim. Prevents LLM from rounding differently or inventing numbers."
    - section: "Zakat Eligibility Constraint"
      lines: "350-352"
      explanation: "If wallet tag is SADAQAH-ELIGIBLE, the prompt PROHIBITS mentioning zakat, asnaf, or any zakat pathway language."
    - section: "Score/Rationale Consistency"
      lines: "366-376"
      explanation: "Low scores (0-9/25) must explain what's MISSING. High scores (18-25/25) can highlight strengths. Prevents praise for poor performance."
    - section: "Citation Rules"
      lines: "381-385"
      explanation: "Every citation must reference a numbered source from the provided list. Cannot invent sources or use numbers beyond the list."

rich_narrative_v2:
  name: "Rich Narrative Generator (v2)"
  category: "narrative_generation"
  description: "Investment memo-style deep analysis with inline citations. Premium content for logged-in users."
  status: "active"
  source_file: "src/llm/prompts/rich_narrative_v2.txt"
  annotations:
    - section: "Immutable Fields"
      lines: "14-21"
      explanation: "Headline, at-a-glance, AMAL scores, and zakat guidance are inherited from baseline. Prevents inconsistency between tiers."
    - section: "Wikipedia-Style Citations"
      lines: "27-39"
      explanation: "Uses <cite id='N'>linked claim text</cite> format, not academic-style trailing [1]. Makes citations readable inline."
    - section: "Deep Linking Requirements"
      lines: "44-56"
      explanation: "URLs must point to EXACT location where evidence is found. Homepage links are rejected. Anchor fragments (#section) required."
    - section: "Deprecated URLs"
      lines: "58-60"
      explanation: "GuideStar URLs are explicitly banned (site deprecated). Candid profile URLs banned (require internal IDs we don't have)."
    - section: "Source Priority"
      lines: "94-97"
      explanation: "PDFs (990, annual reports) highest confidence. Then ProPublica (IRS data). Then rating agencies. Website claims lowest."

# =============================================================================
# FINANCIAL EXTRACTION
# =============================================================================

charity_navigator_financials:
  name: "Charity Navigator Financial Extractor"
  category: "data_extraction"
  description: "Extracts detailed financial metrics from Charity Navigator pages into structured schema."
  status: "active"
  source_file: "src/llm/prompts/charity_navigator_financials.txt"
  annotations:
    - section: "Financial Fields"
      lines: "1-20"
      explanation: "Extracts revenue, expenses, assets, liabilities, and efficiency ratios from CN's financial pages."
    - section: "Score Extraction"
      lines: "25-35"
      explanation: "Extracts CN's star rating, financial score, accountability score, and beacon awards."

# =============================================================================
# CATEGORY CALIBRATION PROMPTS (Planned - Not Yet Wired)
# =============================================================================

category_advocacy_civic:
  name: "Advocacy & Civic Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for advocacy and civic engagement organizations. Adjusts benchmarks for lobbying-adjacent work."
  status: "planned"
  source_file: "src/llm/prompts/categories/ADVOCACY_CIVIC.txt"
  annotations:
    - section: "Evidence Standards"
      lines: "1-20"
      explanation: "Policy change is harder to measure than direct services. Evidence grades adjusted for systemic change timelines."

category_basic_needs:
  name: "Basic Needs Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for food, shelter, and emergency assistance organizations."
  status: "planned"
  source_file: "src/llm/prompts/categories/BASIC_NEEDS.txt"
  annotations:
    - section: "Cost Benchmarks"
      lines: "1-20"
      explanation: "Lower cost-per-beneficiary expected due to economies of scale. Food banks vs. specialized services."

category_civil_rights_legal:
  name: "Civil Rights & Legal Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for civil rights and legal aid organizations."
  status: "planned"
  source_file: "src/llm/prompts/categories/CIVIL_RIGHTS_LEGAL.txt"
  annotations:
    - section: "Impact Measurement"
      lines: "1-20"
      explanation: "Legal victories and policy changes are valid impact metrics. Cases won, people represented, laws changed."

category_education_higher_religious:
  name: "Higher Religious Education Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for Islamic seminaries, religious colleges, and scholarship programs."
  status: "planned"
  source_file: "src/llm/prompts/categories/EDUCATION_HIGHER_RELIGIOUS.txt"
  annotations:
    - section: "Outcome Metrics"
      lines: "1-20"
      explanation: "Graduates, scholars produced, curriculum development. Religious impact harder to quantify."

category_education_international:
  name: "International Education Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for education programs in developing countries."
  status: "planned"
  source_file: "src/llm/prompts/categories/EDUCATION_INTERNATIONAL.txt"
  annotations:
    - section: "Cost Adjustments"
      lines: "1-20"
      explanation: "Lower absolute costs expected due to purchasing power. But security costs in conflict zones add 1.5x multiplier."

category_education_k12_religious:
  name: "K-12 Religious Education Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for Islamic schools, weekend programs, and youth education."
  status: "planned"
  source_file: "src/llm/prompts/categories/EDUCATION_K12_RELIGIOUS.txt"
  annotations:
    - section: "Evidence Standards"
      lines: "1-20"
      explanation: "Student outcomes, graduation rates, Quran memorization metrics. Community survey data accepted."

category_environment_climate:
  name: "Environment & Climate Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for environmental conservation and climate action organizations."
  status: "planned"
  source_file: "src/llm/prompts/categories/ENVIRONMENT_CLIMATE.txt"
  annotations:
    - section: "Impact Timelines"
      lines: "1-20"
      explanation: "Long-term impact (decades). Carbon offsets, conservation acres, species protected as valid metrics."

category_humanitarian:
  name: "Humanitarian Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for emergency response and humanitarian aid organizations."
  status: "planned"
  source_file: "src/llm/prompts/categories/HUMANITARIAN.txt"
  annotations:
    - section: "Conflict Zone Adjustments"
      lines: "1-20"
      explanation: "1.5x cost multiplier for security, logistics in active conflict zones (Gaza, Syria, Yemen, Sudan)."

category_media_journalism:
  name: "Media & Journalism Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for Muslim media outlets, journalism, and content creation."
  status: "planned"
  source_file: "src/llm/prompts/categories/MEDIA_JOURNALISM.txt"
  annotations:
    - section: "Reach Metrics"
      lines: "1-20"
      explanation: "Audience size, engagement, content quality. Different from direct service delivery."

category_medical_health:
  name: "Medical & Health Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for healthcare, mental health, and medical assistance organizations."
  status: "planned"
  source_file: "src/llm/prompts/categories/MEDICAL_HEALTH.txt"
  annotations:
    - section: "Cost-Per-Beneficiary"
      lines: "1-20"
      explanation: "Higher costs expected due to medical equipment, trained staff. Quality-adjusted life years (QALYs) as metric."

category_philanthropy_grantmaking:
  name: "Philanthropy & Grantmaking Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for donor-advised funds, foundations, and grantmaking organizations."
  status: "planned"
  source_file: "src/llm/prompts/categories/PHILANTHROPY_GRANTMAKING.txt"
  annotations:
    - section: "Pass-Through Model"
      lines: "1-20"
      explanation: "Different efficiency model - admin costs support grant decisions, not direct service. Grant outcomes matter."

category_religious_congregation:
  name: "Religious Congregation Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for mosques and Islamic centers with 501(c)(3) status."
  status: "planned"
  source_file: "src/llm/prompts/categories/RELIGIOUS_CONGREGATION.txt"
  annotations:
    - section: "Data Limitations"
      lines: "1-20"
      explanation: "Many file Form 990-N (e-Postcard). Limited financial data. Community surveys and membership as metrics."

category_religious_outreach:
  name: "Religious Outreach Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for dawah, convert support, and Islamic education outreach."
  status: "planned"
  source_file: "src/llm/prompts/categories/RELIGIOUS_OUTREACH.txt"
  annotations:
    - section: "Impact Measurement"
      lines: "1-20"
      explanation: "Shahada numbers, community integration support, educational materials distributed."

category_research_policy:
  name: "Research & Policy Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for think tanks, research institutions, and policy advocacy."
  status: "planned"
  source_file: "src/llm/prompts/categories/RESEARCH_POLICY.txt"
  annotations:
    - section: "Academic Output"
      lines: "1-20"
      explanation: "Publications, citations, policy briefs, testimony. Long timelines for policy change."

category_social_services:
  name: "Social Services Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for family services, counseling, and community support organizations."
  status: "planned"
  source_file: "src/llm/prompts/categories/SOCIAL_SERVICES.txt"
  annotations:
    - section: "Service Delivery"
      lines: "1-20"
      explanation: "Clients served, case outcomes, family stability metrics. Cost-per-case benchmarks."

category_womens_services:
  name: "Women's Services Category Calibration"
  category: "category_calibration"
  description: "Calibrates scoring for women's shelters, empowerment programs, and gender-specific services."
  status: "planned"
  source_file: "src/llm/prompts/categories/WOMENS_SERVICES.txt"
  annotations:
    - section: "Safety Metrics"
      lines: "1-20"
      explanation: "Women housed, restraining orders obtained, job placement rates. Confidentiality constraints on reporting."
